# ğŸ§  RAGÂ Scholar

A **Retrievalâ€‘AugmentedÂ Generation (RAG)** research assistant that lets you query Wikipedia orÂ arXiv papers through an LLMâ€‘powered QA interface.  The project ships with both a **Streamlit** GUI and a **commandâ€‘lineÂ tool** and is built entirely on openâ€‘source components.

---

## Keyâ€¯Features

| Capability                         | Details                                                                                                                     |
| ---------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| **Multisource ingestion**    | Choose between Wikipedia search or arXiv API.                                                                               |
| **Local vector store**       | FAISS index persisted under `data/` for fast, offline retrieval.                                                          |
| **Pluggable embeddings**     | Default:`sentenceâ€‘transformers/allâ€‘MiniLMâ€‘L6â€‘v2`, but any SBERT model will work.                                      |
| **Model flexibility**        | Supports Huggingâ€¯Face seq2seq (e.g. Flanâ€‘T5)**or** causal LMs (e.g. Falconâ€‘7Bâ€‘Instruct); optional OpenAI backend. |
| **Custom prompt**            | Fewâ€‘shot prompt with context injection + citations.                                                                        |
| **Interactive UI**           | Streamlit frontâ€‘end with live source inspectors.                                                                           |
| **Robust CLI**               | Batch or interactive shell for headless environments.                                                                       |
| **Configurable & stateless** | Reâ€‘initialise knowledge base on demand or reuse a saved index.                                                             |

---

## SystemÂ Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User UI   â”‚â”€â”€â”   â”‚ LangChain RAG Chain â”‚â”€â”€â”   â”‚ LargeÂ Languageâ”‚
â”‚ (Streamlit â”‚  â”‚   â”‚  RetrievalQA (LLM +â”‚  â”‚   â”‚   Model       â”‚
â”‚   or CLI)  â”‚  â”‚   â”‚   Retriever)       â”‚  â”‚   â”‚ (HF / OpenAI) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                            â”‚
                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                â””â”€â”€â–ºâ”‚   FAISS VectorStore  â”‚â—„â”˜
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–²
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Document Loaders (Wiki /   â”‚
                â”‚  arXiv) + TextÂ Splitter     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## QuickÂ Start

### 1Â Â·Â Clone &Â Install

```bash
git clone https://github.com/yourâ€‘handle/ragâ€‘scholar.git
cd ragâ€‘scholar

python -m venv .venv && source .venv/bin/activate  # on Windows: .\.venv\Scripts\activate

# Install all dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

### 2Â Â·Â EnvironmentÂ Variables

Create a `.env` file (read via **pythonâ€‘dotenv**) and set:

```ini
# Optional â€” only if you tick "Use OpenAI" in the UI
OPENAI_API_KEY="skâ€‘..."

# For private HuggingÂ Face models
HUGGINGFACEHUB_API_TOKEN="hf_..."
```

### 3Â Â·Â Launch the StreamlitÂ app

```bash
streamlit run app.py
```

The first launch downloads documents, builds embeddings, and writes a FAISS index to `data/vector_db_<source>_<topic>/`. Subsequent runs reuse the index unless you tick â€œInitialize/ReloadÂ KnowledgeÂ Baseâ€.

### 4Â Â·Â CLI Usage (headless servers)

```bash
python rag_scholar.py \
    --source wikipedia \
    --topic "quantum computing" \
    --question "What is a topological qubit?"
```

Add `--openai` to route generation through the OpenAIÂ API.

---

## Configuration Flags

| Flag           | Type                        | Default                | Description                                |
| -------------- | --------------------------- | ---------------------- | ------------------------------------------ |
| `--source`   | `wikipedia` Â·Â `arxiv` | `wikipedia`          | Data corpus to ingest.                     |
| `--topic`    | *str*                     | `"machine learning"` | Search query (Wiki) or arXiv title filter. |
| `--openai`   | toggle                      | *False*              | Switch from HuggingÂ Face to OpenAI LLM.   |
| `--ui`       | toggle                      | *False*              | Run Streamlit interface.                   |
| `--question` | *str*                     | â€“                     | Ask a single question in CLI mode.         |

---

## RAGÂ Pipeline Internals

1. **Loader** pulls raw documents via *LangChainÂ loaders*.
2. **RecursiveCharacterTextSplitter** chunks text (`chunk_size=1000`, overlapÂ 100).
3. **CustomSentenceTransformerEmbeddings** encodes chunks â†’ 384â€‘d vectors.
4. **FAISS** stores vectors locally; persisted across sessions.
5. **RetrievalQA** fetches topâ€‘*k* (`k=4`) chunks, injects into a custom prompt, and streams completion from the selected LLM.
6. Sources are returned alongside answers for full transparency.

---

## RepositoryÂ Layout

```
.
â”œâ”€â”€ app.py              # Streamlit frontâ€‘end
â”œâ”€â”€ requirements.txt    # Python deps (â‰ˆÂ pythonÂ 3.10+)
â”œâ”€â”€ data/               # Persisted FAISS indices
â””â”€â”€ README.md           # (you are here)
```

---

## Logging

`logging` is configured at **INFO** level by default.

```
2025â€‘06â€‘14Â 12:34:56,789Â â€‘ RAGScholarÂ â€‘ INFOÂ â€‘ LoadedÂ 256Â documents
```

Set the environment variable `LOGLEVEL=DEBUG` for verbose output.

---

## DevelopmentÂ Notes

- **GPU**:  If you have CUDA installed, change `model.to("cuda")` in `RAGScholar.__init__()` to unlock GPU inference.
- **Vector DB schema changes**:  A mismatching index triggers a rebuild; see `load_vectorstore()`.
- **Adding new data sources**:  Implement another `Loader` and extend `source_type` logic.

---

Made withÂ LangChainÂ v0.2.0
